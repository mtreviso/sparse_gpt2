# Sparse GPT-2

Simple implementation of GPT-2 with sparse attention mechanism using the [alpha-entmax](https://github.com/deep-spin/entmax/) transformation.


## How to use

Install the requirements: `pip install -r requirements.txt`.

And then run the following command:
```bash
python run_generation.py
```


